1. what is SVM:
    svm is supervised machine learning algorithm which used for both classification and regresssion problems
    SVMs is based on the idea of finding a hyperplane that best divides a dataset in two classes.
    How to finding the  hyperplane is depends on the largest margin.

1.2 what is logistic regresssion:
	in logistic regression, we take the output of the linear function and squash teh value within the range of [0,1] using the sigmoid function(logistic function). The Sigmoid Function is an S-shape curve that can take any real-value number and map it into a value between the range 0 and 1, but never exactly at those limits.
	we want to amximize teh likelihood that a randome data point gets classified correctly.

	you can maximize the likelihood using different methods like an optimization algorithm(Newton's Method, Gradient Descent etc)

1.3 Different between logistic and linear regression:
	the different is that logistic regression gives you a discrete outcome but linear regression gives a continuous outcome. 

2. SVM vs Logistic:
	* SVM try to maximize the margin between the closest support vectors while LR the posterior class probability. Thus, SVM find a solution which is as fare as possible for teh two categories while LR has not this property.

	* LR is more sensitive to outliers than SVM because the cost function of LR diverges faster than those of SVM.

	* Logistic Regressio produces probabilistic values while SVM produces 1 or 0. So in a few words LR makes not absolute prediction and it does not assume data is enough to give a final decision. This maybe be good property when what we want is an estimation or we do not have high confidence into data.

	Usually, in order to get discrete values 1 or 0 for the LR we can say that when a function value is greater than threshold we classify as 1 and when a function value is smaller than threshold we calssify as 0.

	In general:
	* SVM tries to find the widest possible separating margin, while Logistic Regression optimizes the log likelihood funciton, whith probabilities modeled by the sigmoid fucntion.
	* SVM extends by using kernel tricks, transforming datasets into rich features space, so that complex problems can be still dealt with in the same "linear" fashion in the lifted hyper space.




3. Backpropagation:
    for each training instance the backpropagation algorithm first makes a prediction(forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection(reverse pass), and finally slightly tweaks the connection weights to reduce the error.(Gradient Descent step)

4. ReLU(rectified linear unit)function:
    Most importantly, the fact that it does not have maximum output value also helps reduced likelihood of vanishing gradien.

    The issue is that all the negative values becomes zero immediately which descreases the ability of the model to fit or train from data properly.
    
    Because of the horizontal line in ReLu(for negative X), teh graident can go towards 0. For activations in that region of Relu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/input(simply because gradient is 0, nothing changes) This is called dying ReLu problem. This problem can cause several neurons to just die and not respond making a substantial part of the network passive. There are variations in ReLu to mitigate this issue by simply making the horizontal line into non horizontal component. For example y = 0.01x for x < 0 will make it slightly inclined line rather than horizontal line. This is leakly Relu. There are other variations too. THe main idea is to let the gradient be non zero and recover during training eventually.

    Its limiation is that it should only be used within Hidden layers of a Neural Network Model
    Hence for output layers we should use a Softmax function for a Classification problem to compute the probabilites for the class, and for a regressin problem it should simply use a linear function.

    * Pros:
    	1. It avoid and rectifies vanishing gradient problem
    	2. ReLU is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations
    * Cons:
    	1. One of its limitation is that it should only be used with hidden layers of a Neural Network Model
    	2. Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.
    	3. In another words, for activations in region(x<0) of Relu, gradient will be 0 because of which the weight will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/input(simply because gradient is 0, nothing changes). This is called dying Relu problem.
    	4. The range of Relu is [0, inf). This means it can blow up the activation.

4.2 ELU exponential Linear Unit:
	Exponential linear unit is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.

	ELU is very similar to RELU except negative inputs. They are both in identity function form for non-negative inputs.

	* Pros:
		1. Unlike to Relu, ELU can produce negative outputs.
		2. ELU becomes smooth slowly until its output equal to -a whereas RELU sharply smoothes.
		3. ELU is a strong alternative to RELU.


5.  Vanishing Gradients:
    Vanishing gradient problem depends on the choice of the activation funciton. Many common activation functions(e.g sigmoid or tanh)'squash' their input into a very small output range in a very non-linear fashion. For example, sigmoid maps the real number line onto a "small" range of [0,1]. As a result, there are large regions of the input space which are mapped to an extremely small range. in these regions of the input space, even a large change in the input will produce a small change in the output- hence the graident is small.

    This becomes much worse when we stack multiple layes of such non-linearities on top of each other. For instance, first layer will map a large input region to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on. As a result, even a large change in the parameter of the first layer doesn't change the output much.

    We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. A popular choice is Rectified linear Unit which maps x to max(0, x).



     The bigger the input(in absolute value) the smaller the gradient of the sigmoid function. But, probably an even more important effect is that the derivative of the sigmoid function is Always smaller than one. In fact it is at most 0.25!?

     The down side of this is that if you have many layers, you will multiply these gradients, and the product of many smaller than 1 values goes to zero very quickly.

     Since the state of the art of for Deep learning has shown that more layers helps a lot, then this disadvantage of the Sigmoid function is a game killer. You just can't do deep learning with sigmoid.

     On the other hand the gradient of the ReLu function is either 0 for a < 0 or 1 for a > 0. That means that you can put as many layers as you like, because multiplying the gradients will neither vanish nor explode.




 6. Sigmoid Activation function:
    * Vanishing gradient problem
    * Secodly, its output isn't zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.
    * sigmoids saturate and kill gradients.
    * Sigmoid have slow convergence. 
             Convergence is usually faster if the average of each input variable over the training set is close to zero.

    advantage:
       the output of  the activaton function is always going to be in range (0,1) compare to (-inf, inf) of linear function. So we have our activation bound in a range. Nice, it won't blow up the activations then.

6.2 Tanh Hyperbolic tangent function:
	Tanh squashed a real-valued number to the range [-1,1]. It's non-linear. But unlike Sigmoid, its output is zero-centered.

	*Pros:
		the gradient is stronger for tanh than sigmoid(derivatives are steeper)
	*Cons:
		Tanh also has the vanishing gradient problem

7. Boosting vs bagging
	bagging is use the same algorithm for every predictor, but to train them on different random subsets of the trianing set.




8. Linear acitvation function:
	A straight line function where activation is proportional to input(which is the weighted sum from neuron)

	* Pros:
		1. It gives a range of activations, so it is not binary activation
		2. We can definitely connect a few neurons together and if more than 1 fires, we could take the max(or softmax) and decide based on that.
	* Cons:
		1. For this function, derivative is a constant That means, the gradient has no relationship with X.
		2. It is a constant gradient and the descent is going to be on constant gradient
		3. If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in the input delta(X)

