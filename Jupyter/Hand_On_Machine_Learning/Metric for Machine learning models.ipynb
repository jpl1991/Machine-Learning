{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the right metric for evaluating Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall or Sensitivity or TPR(True Positive Rate):**\n",
    "Number of items correctly identified as positive out of total true positive -TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity or TNR(True Negative Rate):** Number of item correctly identified as negative out of total negatives-TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision: ** Number of item correctly identified as positive out of total items identified as positive -TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate or Type I Error:** Number of items wrong identified as positive out of total true negatives- FP(FP/TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Negative Rate or Type II Error:** Number of items wrongly identified as negative out of total true postive-FN/(FN+TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score:** It is a harmonic mean of precision and recall given by $F1 = 2*Precision*Recall/(Precision + Recall)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** Percentage of total items classified correctly-\n",
    "(TP+TN)/(N+P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-AUC Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receiver operating characteristic- Area under curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, it is calcualted by area under of sensitivity(TPR) vs FPR(1-specificity). Ideally, we would like to have high sensitivity & high specificity, but in real-world scenarios, there is always a tradeoff between sensitivity & specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important characteristics of ROC-AUC are:\n",
    "\n",
    "    * The value can range from 0 to 1. However auc score of a random classifier for balanced data is 0.5\n",
    "    \n",
    "    * ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-loss is a measurement of accuracy that incorporate the idea of probabilistic confidence given by following expression for binary class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-(y log(p)) + (1-y)log(1-p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes into account the uncertainty of your prediction based on how much it varies from the actual label. In teh worst case, let's say you predicted 0.5 for all the observations. So log-loss will become -log(0.5) = 0.69. Hence, we can say that anything above 0.6 is a very poor model considering the actual probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider Balanced Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * If you care for absolute probabilistic difference, go with log-loss\n",
    "    \n",
    "    * If you care only for the final class prediction and you don't want to tune threshold, go with AUC score\n",
    "    \n",
    "    * F1 score is sensitive to threshold and you would want to tune it first before comparing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider imbalance data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Log-loss is failing in this case because according to log-loss both the models are performing equally. This is beacuse log-loss function is symmetric and does not differentiate between classes.\n",
    "    \n",
    "    * If you care for a class which is smaller in number independent of the fact whether it is positve or negative, go for ROC-AUC score.\n",
    "    \n",
    "    * When you have a small positive class, then F1 score makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which metric should use for muti-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly used metrics for multi-classes are F1 score, Average Accuracy, Log-loss. There is yet no well developed ROC-AUC score for multi--class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**log-loss for multi-class is defined as:**\n",
    "\n",
    "$logloss = - \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M}y_{ij}log(p_{ij})$\n",
    "\n",
    "where,<br>\n",
    "    N: No of Rows in Test set <br>\n",
    "    M: No of Falut Delivery Classes <br>\n",
    "    $Y_{ij}: 1$ \n",
    "    if observation belongs to Class j; else 0 <br>\n",
    "    $p_{ij}:$ Predicted Probability that observation belong to Class j\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Micro-average method, you sum up the individual true positvies, false positives, and false negatives of the system for different sets and then apply them to get the statistic.\n",
    "* In Macro-average, you take the averge of the precision and recall of the system on different sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro-average is preferable if there is a class imbalance problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Regerssion Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean Squared Error(MSE)\n",
    "* Root Mean Squared Error(RMSE)\n",
    "* Mean Absolute Error(MAE)\n",
    "* R squared ($R^2$)\n",
    "* Adjusted R Squared($R^2$)\n",
    "* Mean Square Percentage Error(MSPE)\n",
    "* Mean Absolute Percentage Error(MAPE)\n",
    "* Root Mean Squared Logarithmic Error(RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE basically measures average squared error of our predictions. For each point, it calcualted square difference between the precitions and the target and the average those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantage:** Useful if we have unexpected values that we should care about. Very high or low value that we should pay attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage:** if we make a single very bad prediction, the squaring will make the error even worse and it may skew the metric towards overestimating the model's badness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is just the square root of MSE. The sqaure root is intruduced to make scale of the error to be the same as the scale of targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RMSE = \\sqrt{MSE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, they are similar in terms of their minimizers, every minimizer of MSE is also minimizer for RMSE and vice versa since the square roots is an non-decreasing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(A)>MSE(B)\\Longleftrightarrow RMSE(A)>RMSE(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that, if the target metric is RMSE, we still can compare our models using MSE, since MSE will order the models in the same way as RMSE. Thus we can optimize MSE instead of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inface, MSE is a little bit easier to work with, so everybody uses MSE instead of RMSE. Also a little bit of difference between the two for gradient-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta RMSE}{\\delta \\hat{y_i}} = \\frac{1}{2\\sqrt{MSE}}\\frac{\\delta MSE}{\\delta \\hat{y_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that traveling along MSE gradient is equivalent to traveling along RMSE gradient but with a different flowing rate and the flowing rate depends on MSE score itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though RMSE and MSE are really similar in terms of models scoring, they can be not immediately interchangeable for gradient based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error(MAE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MAE the error is calculated as an average of absolute differences between the target values and the predictions.<br>\n",
    "The MAE is a linear score which means that **all the individual differences are weighted equally** in the average. For example, the difference between 10 and 0 will be twice the difference between 5 and 0. However, same is not sure for RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y_i}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this metric is that it **penalizes huge errors that not as that badly as MSE does.** Thus, it's not that sensitive to outlier as mean square error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE is widely used in finance, where $10 error is usually exactly two times worse than $5 error. On the other hand, MSE metric thinks that $10 error is four times worse than $5 error. MAE is easier to justify than MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important thing about MAE is its gradients with respect to the predictions. The gradiend is a step function and it takes -1 when $\\hat{y_i}$ is smaller than the target and +1 when it is larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that:** if we want to have constant prediction the best one will be the **median value of the target values**. It can be found by setting the derivative of our total error with respect to that constant to zero, and find it from this equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Squared($R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination, or R2, is another metirc use to evalute a model and it is closely realted to MSEï¼Œ but has the advantage of being **scale-free**--it does't matter if the output values are very large or very small, **the R2 is always going to be between -$\\infty$ and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When R2 is negative it means that the model is worse than predicting the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2 = 1 - \\frac{MSE(model)}{MSE(baseline)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE of the baseline is defined as:<br><br>\n",
    "$MSE(baseline) = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\bar{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\bar{y}$ is the mean of the observed $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more clear, this baseline MSE can be thought of as teh MSE that the simplest possible model would get. The simplest possible model would be to always predict teh average of all samples. A value close to 1 indicates a model with close to zero error, and a value close to zero indicates a model very close to baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, R2 is the ratio between how good our model is vs how good is the naive mean model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE vs MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do you have outliers in the data?\n",
    "    - Use MAE\n",
    "* Are you sure they are outliers?\n",
    "    - Use MAE\n",
    "* Or they are just unexpected values we should still care about?\n",
    "    - Use MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is difference between an RMSE and RMSLE(logarithmic error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE incorporates both the variance and the bias of the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is the square root of MSE. In case of unbiased estimator, RMSE is just the square root of variance, which is actually Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Square root of variance is standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of RMSLE, you take the log of the prdictions and actual values. OS basically, what cahnges is the variance that you are measuing. RMSEL is usually used when you don't want to penalize huge differences in the predicated and teh actual values when both predicted and true values are huge numbers.\n",
    "1. if both predicted and actual values are small: RMSE and RMSLE is same.\n",
    "2. If either predicted or the actual value is big: RMSE>RMSLE\n",
    "3. If both predicted and actual values are big:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
