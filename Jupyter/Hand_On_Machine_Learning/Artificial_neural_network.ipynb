{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning a DNN Using Plain TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 =100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2/np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons),stddev=stddev)\n",
    "        W = tf.Variable(init, name = \"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name = \"biases\")\n",
    "        z = tf.matmul(X, W)+b\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, n_output, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution Phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-80aa5e208d67>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/peilinjiang/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/peilinjiang/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/peilinjiang/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/peilinjiang/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/peilinjiang/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 0.9, 'Test accuracy:', 0.9134)\n",
      "(1, 'Train accuracy:', 0.9, 'Test accuracy:', 0.9314)\n",
      "(2, 'Train accuracy:', 0.98, 'Test accuracy:', 0.939)\n",
      "(3, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9465)\n",
      "(4, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9486)\n",
      "(5, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9544)\n",
      "(6, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9559)\n",
      "(7, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9589)\n",
      "(8, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9606)\n",
      "(9, 'Train accuracy:', 0.98, 'Test accuracy:', 0.962)\n",
      "(10, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9632)\n",
      "(11, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9642)\n",
      "(12, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9645)\n",
      "(13, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9665)\n",
      "(14, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9684)\n",
      "(15, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9671)\n",
      "(16, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9689)\n",
      "(17, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9704)\n",
      "(18, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9709)\n",
      "(19, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9706)\n",
      "(20, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9704)\n",
      "(21, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9716)\n",
      "(22, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9718)\n",
      "(23, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9723)\n",
      "(24, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9719)\n",
      "(25, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9719)\n",
      "(26, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9734)\n",
      "(27, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9736)\n",
      "(28, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9739)\n",
      "(29, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9732)\n",
      "(30, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9746)\n",
      "(31, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9749)\n",
      "(32, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9741)\n",
      "(33, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9756)\n",
      "(34, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9747)\n",
      "(35, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9751)\n",
      "(36, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9756)\n",
      "(37, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9755)\n",
      "(38, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9762)\n",
      "(39, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9748)\n",
      "(40, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9751)\n",
      "(41, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9759)\n",
      "(42, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9754)\n",
      "(43, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9752)\n",
      "(44, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9759)\n",
      "(45, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9772)\n",
      "(46, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9768)\n",
      "(47, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9764)\n",
      "(48, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9761)\n",
      "(49, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9765)\n",
      "(50, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9769)\n",
      "(51, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9764)\n",
      "(52, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9773)\n",
      "(53, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9772)\n",
      "(54, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9776)\n",
      "(55, 'Train accuracy:', 1.0, 'Test accuracy:', 0.977)\n",
      "(56, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9762)\n",
      "(57, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9765)\n",
      "(58, 'Train accuracy:', 1.0, 'Test accuracy:', 0.977)\n",
      "(59, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9771)\n",
      "(60, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9774)\n",
      "(61, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9769)\n",
      "(62, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9774)\n",
      "(63, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(64, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(65, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(66, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9778)\n",
      "(67, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(68, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9776)\n",
      "(69, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9775)\n",
      "(70, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9774)\n",
      "(71, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(72, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9772)\n",
      "(73, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9771)\n",
      "(74, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(75, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9781)\n",
      "(76, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9781)\n",
      "(77, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9777)\n",
      "(78, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(79, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9777)\n",
      "(80, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(81, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(82, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(83, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9777)\n",
      "(84, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(85, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(86, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9781)\n",
      "(87, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(88, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(89, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(90, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(91, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(92, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9784)\n",
      "(93, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(94, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(95, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9778)\n",
      "(96, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(97, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9776)\n",
      "(98, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(99, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9784)\n",
      "(100, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(101, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(102, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(103, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(104, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(105, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9776)\n",
      "(106, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(107, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9779)\n",
      "(108, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9777)\n",
      "(109, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(110, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9777)\n",
      "(111, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(112, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9781)\n",
      "(113, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(114, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9778)\n",
      "(115, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(116, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(117, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(118, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(119, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(120, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(121, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(122, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(123, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9784)\n",
      "(124, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(125, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(126, 'Train accuracy:', 1.0, 'Test accuracy:', 0.978)\n",
      "(127, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(128, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(129, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(130, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(131, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9781)\n",
      "(132, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(133, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9784)\n",
      "(134, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9786)\n",
      "(135, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(136, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(137, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(138, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9781)\n",
      "(139, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(140, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9782)\n",
      "(141, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(142, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(143, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(144, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(145, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(146, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(147, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(148, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9783)\n",
      "(150, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9787)\n",
      "(151, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9784)\n",
      "(152, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(153, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(154, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(155, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(156, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(157, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(158, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9784)\n",
      "(159, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(160, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9786)\n",
      "(161, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(162, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(163, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(164, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(165, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(166, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(167, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(168, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(169, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(170, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9785)\n",
      "(171, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(172, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(173, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(174, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(175, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(176, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(177, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(178, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(179, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(180, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(181, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(182, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(183, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(184, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(185, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(186, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(187, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9788)\n",
      "(188, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(189, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(190, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(191, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(192, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(193, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(194, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(195, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(196, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(197, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(198, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(199, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(200, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(201, 'Train accuracy:', 1.0, 'Test accuracy:', 0.979)\n",
      "(202, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(203, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(204, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(205, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(206, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(207, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(208, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(209, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9791)\n",
      "(210, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(211, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(212, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(213, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(214, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(215, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(216, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(217, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(218, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(219, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(220, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(221, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(222, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(223, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(224, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(225, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(226, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9789)\n",
      "(227, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(228, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(229, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(230, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(231, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9792)\n",
      "(232, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(233, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(234, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(235, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(236, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(237, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(238, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(239, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(240, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(241, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(242, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(243, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(244, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(245, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(246, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(247, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(248, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(249, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9793)\n",
      "(250, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9794)\n",
      "(251, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(252, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(253, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9795)\n",
      "(254, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(255, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(256, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(257, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(258, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(259, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(260, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(261, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(262, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(263, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(264, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(265, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(266, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(267, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(268, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(269, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(270, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(271, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(272, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(273, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(274, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(275, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(276, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(277, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(278, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(279, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(280, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(281, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(282, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(283, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(284, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(285, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(286, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(287, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(288, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(289, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(290, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(291, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(292, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(293, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(294, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(295, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(297, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9802)\n",
      "(298, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(299, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(300, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(301, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(302, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(303, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(304, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(305, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(306, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(307, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(308, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(309, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9797)\n",
      "(310, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9796)\n",
      "(311, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(312, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(313, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(314, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(315, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(316, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(317, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9802)\n",
      "(318, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9802)\n",
      "(319, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(320, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(321, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(322, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(323, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(324, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(325, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(326, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(327, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(328, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(329, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(330, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(331, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(332, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(333, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(334, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(335, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(336, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(337, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(338, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(339, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(340, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(341, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(342, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(343, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(344, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(345, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(346, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(347, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(348, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(349, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(350, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(351, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(352, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(353, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(354, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(355, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(356, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(357, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(358, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9802)\n",
      "(359, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(360, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(361, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(362, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(363, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(364, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(365, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(366, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(367, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(368, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(369, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(370, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(371, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(372, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(373, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(374, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(375, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9798)\n",
      "(376, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(377, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(378, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(379, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(380, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(381, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(382, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(383, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9802)\n",
      "(384, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(385, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9802)\n",
      "(386, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(387, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(388, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(389, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(390, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(391, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(392, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(393, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(394, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(395, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9801)\n",
      "(396, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(397, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9799)\n",
      "(398, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n",
      "(399, 'Train accuracy:', 1.0, 'Test accuracy:', 0.98)\n"
     ]
    }
   ],
   "source": [
    "# train the model:\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X:mnist.test.images,\n",
    "                                           y:mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train,\"Test accuracy:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess,\"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"./my_model_final.ckpt\")\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict = {X:X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Predicted classes:', array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]))\n",
      "('Actual classes:', array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4],\n",
      "      dtype=uint8))\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:\", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a long time, these facts convinced researchers that there was no need to investigate any deeper neural networks. But they overlooked the fact that deep networks have a much higher parameter efficiency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, making them much faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Neurons per Hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the number of neurons in the input and output layers is determined y the type of input and output your task requires.\n",
    "\n",
    "In general you will get more bang for the buck by increasing the number of layers than then number of neurons per layer. Unfortunately, as you can see, finding the perfect amount fo neurons is still somewhat of a black art.\n",
    "\n",
    "A simpler approach is to pick a model with more layers and neurons than you actually need, then use early stopping to prevent if form overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases you can use the ReLU activation function in the hidden layers(or one of its variants). It is a bit faster to compute than other activations, and Gradient Descnet does not get stuck as much on plateaus, thanks to the fact that it does not saturate for large input values(as opposed to the logistic function or the hyerbolic tangent fucntion, which saturate at 1).\n",
    "\n",
    "For the output layer, the softmax activation function is generally a good choice for classification tasks(when teh classes are mutually exclusive). For regression tasks, you can simply use no activation function at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why is it generally preferable to use a logistic Regression classifier rather than a classial Perceptron(i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logsitic Regression calssifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classical Perceptron will converge only if the dataset is linearly separable, and it won't be able to estimate class probabilities. In constrast, a Logistic Regerssion classifier will converger to a good solution even if the dataset is not linearly separable, and it will output class probabilities. If you change teh Perceptron's activation function to the logistic activation function(or the softmax activation function if there are multiple neurons), and if you train it using Gradient Descent(or some other optimization algorithm minimizing the cost function, typically cross entropy), then it becomes equivalent to a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why was the logistic activation function a key ingredient in training the fist MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic activation function was a key ingredient in training the first MLPs because it derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How many neurons do you need in the output layer if you want to classify email into spam or ham? what activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same question for getting your network to predict housing prices as in chapter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify email into spam or ham, you jsut need one neuron in the output layer of a neural network-for example, indicating the probability that the email is spam. You would typically use the logistic activation in the output layer when estimating a probability. If instead you want to tackle MNIST, you need 10 neurons in the output layer, and you must replace the logistic function with the softmax activation function, which can handle multiple classes, outputting one probability per class. Now if you want to neural network predict housing prices like in Chapter 2, then you need one output enuron, using no activation function at all in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is backpropagation and how does it work? what is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parameter(all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagtion step is typically performed thousands or millions of times, using many training batches, until the model parameter converge to values that (hopefully) minimize the cost function. TO compute the gradients, backpropagation use reverse-mode autodiff(although it wasn't called that when backpropagation was invented, and it has been revented several times). Reverse-mode autodiff performs a forward pass though a computation graph, computig every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once. So what's the difference? Well, backpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode autodiff is simply a technique to compute gradients efficiently , and it happens to be used by backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all the hyperparameter you can tweak in basic MLP:<br/>\n",
    "    1. The number of hidden layers,\n",
    "    2. the number of neurons in each hidden layer,\n",
    "    3. the activation function used in each hidden layr and in output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the ReLU activation function is a good default for the hidden layers. For the output layer, in general you will want the logistic activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the MLP overfits the training data, you can try reducing the number of hidden layrs and reducing the number of neurons per hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Training a deep MLP on the MNIST dataset and see if you can get over 98% precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed = 42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name = \"hidden1\",\n",
    "                             activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name = \"hiddent2\",\n",
    "                             activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_output, name = \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learinig_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the directory to write the TensorBoard logs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix =\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%M%D%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\"+ now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"mnist_dnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ceate the Filewriter that we will use to write the TensorBord logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "checkpoint_path = \"./tmp/my_deep_mnist_model.ckpt\"\n",
    "checkpoint_epochs_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_deep_mnist_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy:90.240 \tLoss:0.35380\n",
      "Epoch: 5 \tValidation accuracy:95.120 \tLoss:0.17921\n",
      "Epoch: 10 \tValidation accuracy:96.560 \tLoss:0.12780\n",
      "Epoch: 15 \tValidation accuracy:97.180 \tLoss:0.10326\n",
      "Epoch: 20 \tValidation accuracy:97.480 \tLoss:0.09164\n",
      "Epoch: 25 \tValidation accuracy:97.620 \tLoss:0.08211\n",
      "Epoch: 30 \tValidation accuracy:97.760 \tLoss:0.07871\n",
      "Epoch: 35 \tValidation accuracy:97.840 \tLoss:0.07417\n",
      "Epoch: 40 \tValidation accuracy:97.840 \tLoss:0.07164\n",
      "Epoch: 45 \tValidation accuracy:98.100 \tLoss:0.06748\n",
      "Epoch: 50 \tValidation accuracy:98.020 \tLoss:0.06735\n",
      "Epoch: 55 \tValidation accuracy:98.000 \tLoss:0.06678\n",
      "Epoch: 60 \tValidation accuracy:98.060 \tLoss:0.06728\n",
      "Epoch: 65 \tValidation accuracy:98.200 \tLoss:0.06672\n",
      "Epoch: 70 \tValidation accuracy:98.160 \tLoss:0.06612\n",
      "Epoch: 75 \tValidation accuracy:98.140 \tLoss:0.06645\n",
      "Epoch: 80 \tValidation accuracy:98.180 \tLoss:0.06668\n",
      "Epoch: 85 \tValidation accuracy:98.220 \tLoss:0.06606\n",
      "Epoch: 90 \tValidation accuracy:98.200 \tLoss:0.06740\n",
      "Epoch: 95 \tValidation accuracy:98.200 \tLoss:0.06887\n",
      "Epoch: 100 \tValidation accuracy:98.260 \tLoss:0.06871\n",
      "Epoch: 105 \tValidation accuracy:98.200 \tLoss:0.07068\n",
      "Epoch: 110 \tValidation accuracy:98.160 \tLoss:0.07063\n",
      "Epoch: 115 \tValidation accuracy:98.300 \tLoss:0.07056\n",
      "Epoch: 120 \tValidation accuracy:98.260 \tLoss:0.07301\n",
      "Epoch: 125 \tValidation accuracy:98.280 \tLoss:0.07138\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epochs_path):\n",
    "    #if the checkpoint file exist, restore the model and load the epoch number\n",
    "        with open(checkpoint_epochs_path,\"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch=0\n",
    "        sess.run(init)\n",
    "        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss,\n",
    "                                                                                  accuracy_summary,loss_summary],\n",
    "                                                                                 feed_dict={X:X_valid, y:y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str,epoch)\n",
    "        if epoch % 5 ==0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                 \"\\tValidation accuracy:{:.3f}\".format(accuracy_val*100),\n",
    "                 \"\\tLoss:{:.5f}\".format(loss_val))\n",
    "            saver.save(sess,checkpoint_path)\n",
    "            with open(checkpoint_epochs_path,\"wb\") as f:\n",
    "                f.write(b\"%d\" %(epoch+1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress +=5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(checkpoint_epochs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_deep_mnist_model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X:X_test, y:y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9794"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
