{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential Tools and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **IPython Notebook**\n",
    "\n",
    "\n",
    ">Kernel: IPython is an alternative Python command line shell for interactive computing with lots of useful enhancements over the \"default\" Python interpreter.\n",
    "<br><br>\n",
    ">Jupyter: The browser-based documents Jupyter Notebook is a great environment for scientific computing: Not only to execute code, but also to add informative documentation via Markdown, HTML, LaTeX, embedded images, and inline data plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **numpy**\n",
    "\n",
    ">NumPy, short for Numerical Python. It provides the data structures, algorithms, and library glue needed for most scientific applications involving numerical data in Python. \n",
    "<br><br>\n",
    ">Beyond the fast array-processing capabilities that NumPy adds to Python, one of its primary uses in data analysis is as a container for data to be passed between algorithms and libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Matplotlib**\n",
    "\n",
    ">Matplotlib is the most popular Python library for data visualizations which can generate plots, histograms, power spectra, bar charts, error charts, scatter plots, etc., with just a few lines of code. It will helps us to make easy things easy and hard things possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Seaborn**\n",
    "\n",
    ">Seaborn is a library for making attractive and informative statistical graphics in Python. It aims to make visualization a central part of exploring and understanding data. And it is built on top of matplotlib and tightly integrated with the PyData stack, including support for numpy and pandas data structures and statistical routines from scipy and statsmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Scikit-learn**\n",
    "\n",
    ">Scikit-learn is is the most popular general machine library for Python. It includes a broad range of different classifiers, cross-validation and other model selection methods, dimensionality reduction techniques, modules for regression and clustering analysis, and a useful data-preprocessing module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Scikit-image**\n",
    "\n",
    ">Scikit-image is a collection of algorithms for image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **pip**\n",
    "\n",
    ">The PYPA recomneded tool for installing Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * Install LPP package\n",
    "\n",
    ">$ pip install lpproj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face recognition is one of many applications of digital image processing. It is concerned with the automatic identification of an individual in a digital image. There are many algorithms through which this process can be carried out. One of these algorithms compresses a database of face images and keeps only the data useful for facial approximation. Any face image can then be approximated using only the information contained in the compressed database, even if the image was not in the original database.\n",
    "\n",
    "In my project, using 3 techniques: Principal Component Analysis(PCA), Linear Discriminant Analysis(LDA) and Locality Preserving Projection(LPP). By using LPP, the face images are mapped into a face subspace for analysis. Different from PCA and LDA which effectively see only the Euclidean structure of face space, LPP finds an embedding that preserves local information, and obtains a face subspace that best detects the essential face manifold structure. A face subspace is obtained by Locality Preserving Projection(LPP), each face image in the image space is mapped to a low-dimensional face subspace, which is characterized by a set of feature images, called Laplacianfaces. Face recognition by projecting face images onto a feature space called “eigenfaces” through PCA. LDA, on the other hand, is a supervised learning algorithm. The aim of LDA is  to find a linear combination of features which separate different classes are far away from each other while requiring objects in the same class are close to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, both PCA and LDA only effectively see the Euclidean Structure. They fail to discover the underlying structure which might be a nonlinear submanifold. In 2005, He, Yan, Hu, Niyogi and Zhang proposed a new approach which considers the manifold structure for face analysis through Locality Preserving Projection(LPP). They used LPP to find the face subspaces which they called as Laplacianfaces to achieve face recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the above three methods have been successfully implemented. After the system is trained by the training data, the feature space “eigenfaces” through PCA, the feature space “fisherfaces” though LDA and the feature space “laplacianfaces” through LPP are found using respective methods. Then use the different classifier SVM and KNN to calculate the accuracy, and analysis the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project using Scikit-learn dataset which is \"fetch_lfw_people\". This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website: http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "#Download the data, if not already on disk and load it as numpy array\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "#introspect the images arrays to find the shapes(for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "#for machine learning we use the 2 data directly(as relative pixel\n",
    "#positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" %n_features)\n",
    "print(\"n_classes: %d\" %n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see, in this situation, the dataset totally have 1288 samples, and each sample have 1850 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the dataset as training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1030\n",
      "Testing data size: 258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split into a training set and a test set using a stratified k fold\n",
    "\n",
    "#split into a tranining adn testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 42)\n",
    "\n",
    "print(\"Training data size: \"+ str(X_train.shape[0]))\n",
    "print(\"Testing data size: \"+str(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X = \\{x_1,x_2,...,x_n\\}$ be the matrix containing n face images. Note that each image matrix has been converted into a vector. For example, a $m*n$ matrix is converted into a vector with $m*n$ rows. The subspace “eigenfaces” could be obtained by following the below steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; 1. Computer the mean $\\mu$\n",
    "\\begin{equation}\n",
    "    \\mu = \\frac{1}{n}\\sum_{n}^{i=1}x_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; 2. Compute the Covariance Matrix S\n",
    "\\begin{equation}\n",
    "    S = \\frac{1}{n}\\sum_{n}^{i=1}(x_i-\\mu)(x_i-\\mu)^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; 3. Compute the eigenvectors $\\nu_i$ and eigenvalues $\\lambda_i$\n",
    "\\begin{equation}\n",
    "    S\\nu_i = \\lambda_i\\nu_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; 4. Order the eigenvectors descending by their eigenvalues and select k eigenvectors corresponding to k largest eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above eigenvectors form the subspace , and it is called as “eigenfaces”.  There is still a computational problem left. For images in Sklearn(fetch_lfw_peopledd) database, each image is 50 x 37 meaning that there are 1850 dimensions. Once such faces are applied to calculate the covariance S, S will end up with a 1850 x 1850 matrix which is really huge and almost could not be processed on normal laptops. Here we firt define Y:\n",
    "\\begin{equation}\n",
    "    Y = X -U\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $U=\\{\\mu, \\mu,...,\\mu\\}$. By this way, the original data set is transformed into a data set with zero mean. In order to solve the problem just mentioned, the eigenvectors and eigenvalues of$Y^TY$ is first calculated:\n",
    "\\begin{equation}\n",
    "    Y^TY\\nu_i = \\lambda_i\\nu_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the original eigenvectors are calculaed by a left multiplication of the data matrix Y:\n",
    "\\begin{equation}\n",
    "    YY^T(Y\\nu_i) = \\lambda_i(Y\\nu_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to normalize the eigenvectors just calculated. Finally, the k eigenvectors corresponding to k largest eigenvalues are retrieved and form the subspace W. Also, the column vectors of W are the so-called eigenfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, the number of training samples is far less than the dimensions of each face. As a result, the above method is needed to avoid the computational problem mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implements it in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a subpackage for  PCA. Linear dimensionality reduction using Singular Value Decompostion of the data to project it to a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 150\n",
    "pca = PCA(n_components = n_components, svd_solver='randomized',\n",
    "          whiten = True).fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a supervised learning algorithm meaning that the class labels of training set will be used in the training process. Let X be the matrix containing training face images and $X_i$ be the matrix containing face images belonging to class $i$.\n",
    "\\begin{equation}\n",
    "    X=\\{X_1,X_2, ..., X_C\\}\\\\\n",
    "    X_i = \\{x_1, x_2, ... , x_n\\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter matrices $S_B$ and $S_w$ are calculated as follows:\n",
    "\\begin{equation}\n",
    "    S_B = \\sum_{i=1}^{c}N_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T\\\\\n",
    "    S_w = \\sum_{i=1}^{c}\\sum_{x_j\\in x_i}(x_j-\\mu_i)(x_j-\\mu_i)^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $N_i$ represents the number of training samples belonging to class $i$, $\\mu_i$ represents the mean of training samples belonging to class label $i$ and $\\mu$ and represents the mean of the total samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a subspace where $S_B$ is maximized and $S_w$ is minimized, the following general eigenvalue problem is needed to be solved.\n",
    "\\begin{equation}\n",
    "    S_B\\nu_i = \\lambda_i S_w \\nu_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one is confronted with the difficulty that the within-class scatter matrix $S_w$ is always singular. In order to solve this problem, it is suggested that PCA first projects the image set to a lower dimensional space so that the resulting within-class matrix $S_w$ is no longer singular. After this process, the general eigenvalue problem becomes:\n",
    "\\begin{equation}\n",
    "    S_w^{-1} S_B\\nu_i = \\lambda_i\\nu_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By solving the above eigenvalue problem $W_{fld}$, is obtained which defines a subspace. Please note that there are at most c-1 nonzero generalized eigenvalues for the above formula. Also, note that PCA can directly reduce the dimension of the feature space to N-c because the rank of $S_w$ is at most N-c . Here is N the number of samples while c is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combing the transformation matrix of PCA, the below transformation matrix for LDA is obtained:\n",
    "\\begin{equation}\n",
    "    W=W_{pca}W_{fld}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colum vectors of W are the so-called fisherfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implements it in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a subpackage for LDA. A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
    "\n",
    "The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.\n",
    "\n",
    "The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminat_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(n_components = 150)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Preserving Projections(LPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define some notation to make the rest of the tutorial easier to follow. Each sample in the dataset will be denoted by $\\mathbf{x}_i$. It is common in derivations to consider data points to be represented by column vectors, but it is more natural in programming the algorithms to denote single data points as arrays, which end up as rows in a matrix of we put many of them together. The focus here will be more on programming than the underlying math, so we'll assume our dataset consists of $m$ points in $n$-dimensional space, forming a $m \\times n$ matrix $X$.\n",
    "\n",
    "The steps to perform LPP are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: PCA projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training image set is first projected into the PCA subspace by\n",
    "throwing away several components with small eigenvalues. The reason of PCA projection is to ensure $XDX^T$ is not singular which is helpful to solve a generalized\n",
    "eigenvector problem later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Construct the Adjacency Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An adjacency graph simply describes which input samples are connected. Inputs that are similar should be connected (i.e. they are neighbors), and inputs that are dissimilar should not be connected. A simple way to represent the adjacency graph is an $m \\times m$ matrix, where $m$ is the number of samples you have. If input $\\mathbf{x}_i$ is connected to $\\mathbf{x}_j$, the entry of the matrix at row $i$ and column $j$ is $1$, otherwise it is $0$. The matrix is then symmetric (if $\\mathbf{x}_i$ is connected to $\\mathbf{x}_j$, then $\\mathbf{x}_j$ is connected to $\\mathbf{x}_i$) and it is likely sparse (depending on how neighbors are determined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He and Niyogi describe two methods of determining which samples are close to one another:\n",
    "\n",
    "* $\\epsilon$-neighborhoods: nodes $i$ and $j$ are connected if $||x_i-x_j||_2^2<\\epsilon$\n",
    "* $k$ nearest neighbor: nodes $i$ and $j$ are connected if $i$ is one of $k$ nearest neighbors of $j$ or vice versa (neighbors determined by Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors also note that adjacency can be determined using other (less principled) methods and that LPP will attempt to preserve those neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn has a subpackage (neighbors) for creating graphs and such. This makes it really easy to construct either of these adjacency graphs. By default, we'll cheat and have the adjacency matrices represent the connections by their Euclidean distance, unless mode='connectivity' is passed in. This will make step 3 easier in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "def radius_adj(X, radius, mode='distance'):\n",
    "    A = neighbors.radius_neighbors_graph(X, radius, mode=mode)\n",
    "    return A\n",
    "\n",
    "def kneighbors_adj(X, n_neighbors, mode='distance'):\n",
    "    A = neighbors.kneighbors_graph(X, n_neighbors, mode=mode)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Choose Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the adjacency graph specifies only which nodes are connected, we use a separate matrix to represent how strongly the connected nodes are connected. Like the adjacency graph, the weights are represented by an $m \\times m$ matrix $W$. Again, the authors offer two options for determining the weights:\n",
    "\n",
    "* Heat kernel: $W_{ij} = \\exp \\left( \\frac{||\\mathbf{x}_i - \\mathbf{x}_j ||^2_2}{t} \\right)$ if nodes $i$ and $j$ are connected and zero otherwise\n",
    "\n",
    "* Simple: $W_{ij} = 1$ if nodes $i$ and $j$ are connected and zero otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the heat kernel, we'll rely on the ability of the graph constructors of scikit-learn to return the distances between neighbors. This makes the weight calculations simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple weight option, pass mode='connectivity' to the adjacency matrix construction function and the resulting weight matrix will be the same as the adjacency matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def heat_kernel_weights(dists, param):\n",
    "    W = -dists**2/param\n",
    "    np.exp(W.data, W.data)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute the Eigenmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step involves solving the generalized eigen-problem:\n",
    "\n",
    "$$\n",
    "X^T L X \\mathbf{a} = \\lambda X^T D X \\mathbf{a}\n",
    "$$\n",
    "Where $D$ is the diagonal matrix whose elements are the column-wise sums of $W$, and $L = D - W$ is the graph Laplacian. The nice thing here is that the products of matrices on both sides produce symmetric matrices, so we can use algorithms that are tuned for solving the eigenvalue problem for symmetric matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of eigenvectors (i.e. the matrix whose columns are the eigenvectors arranged in order by increasing eigenvalue) then give us the transformation matrix to take inputs and transform them to the lower dimensional space. That is, we use the first $l$ eigenvectors (first meaning those with the smallest eigenvalues) to perform an embedding to $l$-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigh\n",
    "\n",
    "def compute_mapping(W, l):\n",
    "    D = np.diagflat(W.sum(axis=0))\n",
    "    L = D - W\n",
    "    eigvals, eigvecs = eigh(X.T.dot(L).dot(X),\n",
    "                            X.T.dot(D).dot(X),\n",
    "                            eigvals=(0, l-1))\n",
    "    return eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puting it Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll put things together to make a simple interface to LPP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LocalityPreservingProjection(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=2, adjacency='kneighbors',\n",
    "                 adjacency_param=3, weights='heatkernel',\n",
    "                 kernel_param=0.1):\n",
    "        self.n_components = n_components\n",
    "        self.adjacency = adjacency\n",
    "        self.adjacency_param = adjacency_param\n",
    "        self.weights = weights\n",
    "        self.kernel_param = kernel_param\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.adjacency == 'kneighbors':\n",
    "            adj_func = kneighbors_adj\n",
    "        else:\n",
    "            adj_func = radius_adj\n",
    "            \n",
    "        if self.weights == 'heatkernel':\n",
    "            mode = 'distance'\n",
    "        else:\n",
    "            mode = 'connectivity'\n",
    "            \n",
    "        W = adj_func(X, self.adjacency_param, mode=mode)\n",
    "        \n",
    "        if self.weights == 'heatkernel':\n",
    "            W = heat_kernel_weights(W, self.kernel_param)\n",
    "        \n",
    "        self.components_ = compute_mapping(W, self.n_components)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return X.dot(self.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lpproj import LocalityPreservingProjection \n",
    "\n",
    "lpp = LocalityPreservingProjection(n_components=150, n_neighbors = 15)\n",
    "lpp_train = lpp.fit_transform(X_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines Classificaiton Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the SVM can be formulated as an optimization as following:\n",
    "\\begin{equation}\n",
    "    min_w||w||^2 subject~ to ~y_i(w^Tx_i+b)\\ge 1 ~for~ i =1...N\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quadratic optimization problem subject to linear constraints and there is a unique minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implements it in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a subpackage for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of support vector machines are:\n",
    "\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantages of support vector machines include:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the accuracy using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on above code for fit the svm model, after that we use test data to test the accuarcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_pred = clf.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neigbor Classification Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbor algorithm is a method for classifying objects based on closest training objects: an object is classified according to a majority vote of its neighbors. In this project, KNN plays an important role in recognition phase. The steps of performing KNN are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: find k nearest neighors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a specific test face, it is first projected onto the subspace defined by W, and W is calculated by any methods mentioned above such as PCA, LDA and LPP. Afterward, an array with size is created to keep the K nearest neighbors, and the way to achieve this goal is to go through the whole training set and update the array if there is a training face whose distance is smaller than the largest distance in the original array. At the end of this process, the K nearest neighbors are found. Also, please notice that the training set has already been projected onto the same subspace defined by W. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Classify according to weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the K nearest neighbors are found, a HashMap is created to keep the pairs <label, weight> by going through all the neighbors. If the HashMap happens to meet a new label, <label, 1 / distance> is directly added into the map. Otherwise, an updated version which adds the original weight with 1 / distance is put into the map. After the HashMap is correctly constructed, this program will go through the whole HashMap and find the label associated with the largest weight. This class label will be the label recognized by this system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement it in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a subpackage for knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = NNeighborsClassifiter(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calcualte the accuracy using KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on above code for fit the knn model, we use test data to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_pred = neigh.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implement the project, we can get the accuracy as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   |PCA|LDA|LPP|\n",
    "|----|----|----|----|\n",
    "|SVM|88%|61%|17%|\n",
    "|KNN| 66%|79%|46%|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table we can see Eigenfaces(PCA) has a best the performance under SVM classifier model, and Laplacianfaces(LPP) has really low accuracy under SVM model. Under the KNN classifier model, LPP has a better performance than SVM model, but still not good enough. On the contrary, LDA under KNN model, has a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we change the number of the training set, lets see how the accuracy look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Training set|PCA|LDA|LPP|\n",
    "|-----|-----|-----|-----|\n",
    "|90% data as training|0.67|0.66|0.51|\n",
    "|80% data as training|0.62|0.68|0.46|\n",
    "|70% data as training|0.49|0.71|0.42|\n",
    "|60% data as training|0.49|0.72|0.42|\n",
    "|50% data as training|0.46|0.74|0.36|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: here we use KNN classifier model to caclculate the accuracy when to change to training set size, since KNN is fast, the result of deviation for each algorithm is not too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we can see when the training set decreasing, the accuracy for PCA and LPP is decreasing, but LDA is increasing. From the LDA algorithm, as we know, LDA need both X train and y train parameters to fit the algorithm which is different with PCA and LPP, both algorithm only need X train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: Since choosing the training data is randomized, but size is fixed. Therefore every time run the project will get the different accuracy for each algorithm, but the value is not changed too much and the trend should be same as we analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and furtherwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the result, we can find, PCA performs the best on Scikit-learn database if using SVM classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LDA performs the best on Scikit-learn database if using KNN classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LPP does not perform as expected. This method still could achieve good results with proper setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Furtherwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Change the database to compare each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use of sophisticated and better distance metrics like variance normalized distance may improve the recognition performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/ixjlyons/sklearn-lpp/blob/master/lpp.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. https://github.com/wihoho/FaceRecognition/blob/master/doc/FaceRecognitionReport.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. He, X., Yan, S., Hu, Y., Niyogi, P. and Zhang, H.J. (2005). Face Recognition Using\n",
    "Laplacianfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.\n",
    "27, no. 3, pp. 328-340."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
